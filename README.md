# ğŸ¤– Production LLM Fine-Tuning - Complete Implementation

A production-grade implementation for fine-tuning Large Language Models (LLMs) for domain-specific applications. This project demonstrates end-to-end MLOps practices including data processing, model training with LoRA, evaluation, API deployment, and a user-friendly frontend.

## ğŸ¯ Features

- **Efficient Fine-Tuning**: Uses LoRA (Low-Rank Adaptation) with 4-bit quantization for memory-efficient training
- **Production API**: FastAPI-based REST API for model inference
- **Interactive Frontend**: Streamlit-based web interface for easy interaction
- **RAG Support**: Retrieval-Augmented Generation pipeline for enhanced context
- **Comprehensive Evaluation**: ROUGE scores, exact match, and custom metrics
- **Docker Deployment**: Containerized setup for easy deployment
- **CI/CD Pipeline**: GitHub Actions for automated testing

## ğŸ“ Project Structure

```
domain-llm-finetuning/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Original scraped data
â”‚   â”œâ”€â”€ processed/              # Cleaned and formatted data
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ scraper.py         # Data collection
â”‚       â”œâ”€â”€ preprocess.py      # Cleaning pipeline
â”‚       â””â”€â”€ create_dataset.py  # Format conversion
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base/                   # Downloaded base models
â”‚   â””â”€â”€ finetuned/             # Your trained models
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train.py           # Main training script
â”‚   â”‚   â”œâ”€â”€ config.py          # Training configuration
â”‚   â”‚   â””â”€â”€ utils.py           # Helper functions
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ evaluate.py        # Model evaluation
â”‚   â”‚   â””â”€â”€ metrics.py         # Custom metrics
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ vector_store.py    # ChromaDB setup
â”‚   â”‚   â””â”€â”€ retrieval.py       # RAG pipeline
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ main.py            # FastAPI application
â”‚       â”œâ”€â”€ models.py          # Pydantic schemas
â”‚       â””â”€â”€ inference.py       # Model inference
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py                 # Streamlit UI
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_inference.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb           # Exploratory analysis
â”‚   â”œâ”€â”€ 02_training.ipynb      # Interactive training
â”‚   â””â”€â”€ 03_evaluation.ipynb    # Results visualization
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ training_config.yaml
â”‚   â””â”€â”€ model_config.yaml
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml             # CI/CD pipeline
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Prepare Data

```bash
# Scrape data (or use your own data)
python data/scripts/scraper.py

# Preprocess data
python data/scripts/preprocess.py

# Create train/val/test splits
python data/scripts/create_dataset.py
```

### 3. Train Model

```bash
# Train the model
python src/training/train.py
```

**Note**: Training requires a GPU with at least 16GB VRAM. The model uses 4-bit quantization to reduce memory requirements.

### 4. Evaluate Model

```bash
# Run evaluation
python src/evaluation/evaluate.py
```

### 5. Run API Server

```bash
# Start FastAPI server
uvicorn src.api.main:app --reload
```

The API will be available at `http://localhost:8000`

### 6. Run Frontend

```bash
# Start Streamlit app (in a separate terminal)
streamlit run frontend/app.py
```

The frontend will be available at `http://localhost:8501`

### 7. Docker Deployment

```bash
# Build and run with Docker Compose
docker-compose -f docker/docker-compose.yml up --build
```

## ğŸ“Š API Endpoints

### Health Check
```bash
GET /health
```

### Generate Text
```bash
POST /generate
Content-Type: application/json

{
  "prompt": "What are the symptoms of Type 2 diabetes?",
  "max_tokens": 512,
  "temperature": 0.7,
  "top_p": 0.9,
  "use_rag": false
}
```

## ğŸ”§ Configuration

### Training Configuration

Edit `src/training/config.py` or `configs/training_config.yaml` to customize:

- Model selection
- LoRA parameters (r, alpha)
- Training hyperparameters (epochs, batch size, learning rate)
- Paths and output directories

### Model Configuration

Edit `configs/model_config.yaml` for:

- Model path
- Quantization settings
- Inference parameters

## ğŸ“ˆ Key Performance Indicators

### Training Metrics
- Loss curve convergence
- Perplexity: < 10 (domain-specific)
- Training time: 4-12 hours for 7B model

### Quality Metrics
- ROUGE-L: > 0.5
- Domain accuracy: > 80%
- Human evaluation: 4/5 rating

### Deployment Metrics
- Inference latency: < 500ms
- API uptime: > 99%
- Throughput: > 10 requests/sec

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ -v --cov=src --cov-report=html
```

## ğŸ“ Development

### Code Formatting

```bash
# Format code
black src/ tests/

# Lint code
flake8 src/ tests/
```

## ğŸ³ Docker

### Build Image

```bash
docker build -f docker/Dockerfile -t domain-llm-api .
```

### Run Container

```bash
docker run -p 8000:8000 --gpus all domain-llm-api
```

## ğŸ“š Documentation

- [Training Guide](docs/training.md)
- [API Documentation](http://localhost:8000/docs) (when API is running)
- [Deployment Guide](docs/deployment.md)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Hugging Face for transformers and datasets
- PEFT library for LoRA implementation
- FastAPI for the API framework
- Streamlit for the frontend framework

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**Note**: This is a template project. Replace placeholder data and configurations with your actual domain-specific data and requirements.

