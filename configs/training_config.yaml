# Training Configuration
model:
  name: "mistralai/Mistral-7B-Instruct-v0.2"
  
lora:
  r: 32
  alpha: 64
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

training:
  epochs: 3
  batch_size: 4
  gradient_accumulation: 4
  learning_rate: 2e-5
  max_length: 1024
  warmup_steps: 100
  logging_steps: 10
  eval_steps: 500
  save_steps: 500
  save_total_limit: 3

paths:
  output_dir: "./models/checkpoints"
  final_model_path: "./models/finetuned/medical-llm"
  train_data: "./data/processed/train.json"
  eval_data: "./data/processed/eval.json"

wandb:
  project_name: "medical-llm-finetuning"
  run_name: "mistral-7b-medical-v1"
  enabled: true

